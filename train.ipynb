{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cards_class import CardsDataset\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model import SSD300, MultiBoxLoss\n",
    "from utils import unpackBoundigBox, showBatch, collate_fn, train, save_checkpoint, adjust_learning_rate\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'data/images'\n",
    "anotations_path = 'data/txt_cards'\n",
    "labels_path = 'data/general_labels/classes.txt'\n",
    "model_path = 'data/models/cards_2.pth.tar'\n",
    "checkpoint = None\n",
    "\n",
    "images_path = '/cards/3/data/images'\n",
    "anotations_path = '/cards/3/data/txt_cards'\n",
    "labels_path = '/cards/3/data/general_labels/classes.txt'\n",
    "model_path = 'data/cards_2.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset lenght:\n 642\nClasses:\n {0: 'background', 1: 'a_heart', 2: 'a_diamond', 3: 'a_club', 4: 'a_spade', 5: '2_heart', 6: '2_diamond', 7: '2_club', 8: '2_spade', 9: '3_heart', 10: '3_diamond', 11: '3_club', 12: '3_spade', 13: '4_heart', 14: '4_diamond', 15: '4_club', 16: '4_spade', 17: '5_heart', 18: '5_diamond', 19: '5_club', 20: '5_spade', 21: '6_heart', 22: '6_diamond', 23: '6_club', 24: '6_spade', 25: '7_heart', 26: '7_diamond', 27: '7_club', 28: '7_spade', 29: '8_heart', 30: '8_diamond', 31: '8_club', 32: '8_spade', 33: '9_heart', 34: '9_diamond', 35: '9_club', 36: '9_spade', 37: '10_heart', 38: '10_diamond', 39: '10_club', 40: '10_spade', 41: 'j_heart', 42: 'j_diamond', 43: 'j_club', 44: 'j_spade', 45: 'q_heart', 46: 'q_diamond', 47: 'q_club', 48: 'q_spade', 49: 'k_heart', 50: 'k_diamond', 51: 'k_club', 52: 'k_spade', 53: 'joker'}\nNumber of classes:\n 54\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CardsDataset(images_path, anotations_path, labels_path)\n",
    "num_classes = len(list(train_dataset.labels.keys()))\n",
    "print('Dataset lenght:\\n', len(train_dataset))\n",
    "print('Classes:\\n', train_dataset.labels)\n",
    "print('Number of classes:\\n', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Availabele GPU:\n cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Availabele GPU:\\n', device)"
   ]
  },
  {
   "source": [
    "### Learning Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "# number of iterations to train\n",
    "iterations = 15000\n",
    "# number of workers for loading data in the DataLoader\n",
    "workers = 4\n",
    "# print training status every __ batches\n",
    "print_freq = 50\n",
    "lr = 1e-4  # learning rate\n",
    "decay_lr_at = [8000, 10000]  # decay learning rate after these many iterations\n",
    "decay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\n",
    "momentum = 0.9  # momentum\n",
    "weight_decay = 5e-4  # weight decay\n",
    "grad_clip = None  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded VGGBase base model.\n",
      "Number of epochs:\n",
      " 93\n",
      "Decaying learning rate at epochs:\n",
      " [50, 62]\n",
      "Epoch: [0][0/160]\tBatch Time 13.506 (13.506)\tData Time 2.817 (2.817)\tLoss 30.0101 (30.0101)\t\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10fd84a60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/acano/.virtualenvs/py3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/acano/.virtualenvs/py3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e85ded4d951b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# One epoch's training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     train(train_loader=train_loader, model=model,criterion=criterion,\n\u001b[0m\u001b[1;32m     49\u001b[0m             optimizer=optimizer,epoch=epoch, print_freq=print_freq, grad_clip=grad_clip)\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Playing_Cards/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, print_freq, grad_clip)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Backward prop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Clip gradients, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model or load checkpoint\n",
    "if checkpoint is None:\n",
    "    start_epoch = 0\n",
    "    model = SSD300(n_classes=num_classes)\n",
    "    # Initialize the optimizer, with twice the default learning rate for biases, as in the original Caffe repo\n",
    "    biases = list()\n",
    "    not_biases = list()\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param_name.endswith('.bias'):\n",
    "                biases.append(param)\n",
    "            else:\n",
    "                not_biases.append(param)\n",
    "    optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                                lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print('Loaded checkpoint from epoch %d.' % start_epoch)\n",
    "    model = checkpoint['model']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "\n",
    "# Move to default device\n",
    "model = model.to(device)\n",
    "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=workers,  collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "\n",
    "epochs = iterations // (len(train_dataset) // batch_size)\n",
    "# epochs = 1\n",
    "print('Number of epochs:\\n', epochs)\n",
    "decay_lr_at = [it // (len(train_dataset) // batch_size) for it in decay_lr_at]\n",
    "print('Decaying learning rate at epochs:\\n', decay_lr_at)\n",
    "\n",
    "\n",
    "    # Epochs\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    # Decay learning rate at particular epochs\n",
    "    if epoch in decay_lr_at:\n",
    "        adjust_learning_rate(optimizer, decay_lr_to)\n",
    "\n",
    "    # One epoch's training\n",
    "    train(train_loader=train_loader, model=model,criterion=criterion,\n",
    "            optimizer=optimizer,epoch=epoch, print_freq=print_freq, grad_clip=grad_clip)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 2 == 0 and model_path is not None:\n",
    "        save_checkpoint(model_path,epoch, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}